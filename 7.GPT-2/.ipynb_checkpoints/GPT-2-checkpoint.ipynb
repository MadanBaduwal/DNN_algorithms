{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdd893b",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* https://jaketae.github.io/study/gpt/\n",
    "* https://github.com/karpathy/minGPT\n",
    "* https://forums.fast.ai/t/need-help-with-implementing-gpt-2-from-scratch/62189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7b0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn # https://pytorch.org/docs/stable/nn.html#sparse-layers\n",
    "import torch.nn.functional as F # https://pytorch.org/docs/stable/nn.functional.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6888fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    attn_dropout = 0.1\n",
    "    embed_dropout = 0.1\n",
    "    ff_droupout = 0.1 \n",
    "    \n",
    "    def __init__(self,vocab_size,ma_len,**kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        for key,value in kwarg.items():\n",
    "            setattr(self,key,value)\n",
    "class GPT1Config(GPTConfig):\n",
    "    num_heads = 12 \n",
    "    num_blocks = 12\n",
    "    embed_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        embed_ dim = config.max_len\n",
    "        self.max_len = config.max_len\n",
    "        self.tok_embed = nn.Embedding(config.vocab_size, embed_size\n",
    "                                     )\n",
    "        \n",
    "        sef.pos_embed = nn.Parameter(torch.zeros(1,config.max_len,embed_dim)\n",
    "                                    )\n",
    "        self.dropout = nn.Dropout(config.embed_dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.num_blocks)]\n",
    "                                   )\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim,config.vocab_size)\n",
    "        \n",
    "    def forward(self,x,target = None):\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
    "        \n",
    "        tok_embedding = self.tok_embed(x)\n",
    "        \n",
    "        pos_embedding = self.pos_embed[:,:seq_len,:]\n",
    "        \n",
    "        x =  self.dropout(tok_embedding + pos_embedding)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc02bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
